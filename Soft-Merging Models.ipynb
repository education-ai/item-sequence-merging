{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d89c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Evaluates Combined Models\n",
    "Prototype for Experiments\n",
    "\"\"\"\n",
    "__author__ = [\"Leo S. R端dian\"]\n",
    "__copyright__ = \"2024, R端dian\"\n",
    "__credits__ = [\"Leo S. R端dian\"]\n",
    "__license__ = \"CC BY-NC-SA\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = [\"Leo S. R端dian\"]\n",
    "__email__ =[\"ruediasy@informatik.hu-berlin.de\"]\n",
    "__status__ = \"Prototype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955e640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy import hstack\n",
    "from numpy import argmax\n",
    "from matplotlib import pyplot\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import Conv1DTranspose\n",
    "from keras.layers import Conv1D, MaxPooling1D, RepeatVector\n",
    "from keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from numpy.random import randint\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Input, TimeDistributed,ZeroPadding1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dummy model\n",
    "\n",
    "a = ['B','P','SCD','SCE','MC','H','W','SP','T_9','T_10']\n",
    "b = list(range(0,50)) # 50: items: 0-49\n",
    "b_cluster = [\n",
    "    [0,1,2,3],\n",
    "    [4,5],\n",
    "    [6,7,8,9],\n",
    "    [10,11,12,13],\n",
    "    [14,15,16],\n",
    "    [17,18],\n",
    "    [19,20],\n",
    "    [21,22],\n",
    "    [23,24,25],\n",
    "    [26,27,28,29],\n",
    "    [30,31,32],\n",
    "    [33,34,35,36],\n",
    "    [37,38,39,40],\n",
    "    [41,42],\n",
    "    [43,44],\n",
    "    [45,46,47],\n",
    "    [48,49],\n",
    "]\n",
    "c = [0,1] # knowledge binary level: 1:'known','0:unknown'\n",
    "d = ['pass','fail'] # performance\n",
    "e = [0,1] # groups preference level\n",
    "f = [0,1] # collapsing parameter\n",
    "\n",
    "def convert_to_hotvector(X,a):\n",
    "    mapping = {}\n",
    "    for x in range(len(a)):\n",
    "      mapping[a[x]] = x\n",
    "   \n",
    "    # integer representation\n",
    "    for x in range(len(X)):\n",
    "      X[x] = mapping[X[x]]\n",
    "    \n",
    "    for x in range(len(a)):\n",
    "        \n",
    "        X.append(x)\n",
    "    C = to_categorical(X, num_classes=len(a))\n",
    "    C = C[:-(len(a))]\n",
    "    return C,mapping\n",
    "\n",
    "def getVector(s1,s2,s3,s4,s5):\n",
    "    global a,b,c,d,e\n",
    "    global mapA,mapB,mapC,mapD,mapE\n",
    "    \n",
    "    A_c,mapA = convert_to_hotvector(list(s1),a) \n",
    "    B_c,mapB = convert_to_hotvector(list(s2),b) \n",
    "    C_c,mapC = convert_to_hotvector(list(s3),c)\n",
    "    D_c,mapD = convert_to_hotvector(list(s4),d) \n",
    "    E_c,mapE = convert_to_hotvector(list(s5),e) \n",
    "    \n",
    "    return [A_c,B_c,C_c,D_c,E_c]\n",
    "\n",
    "def getVectorY(s1,s2,s3,s4,s5):\n",
    "    global a,b,c,d,e\n",
    "    global mapA,mapB,mapC,mapD,mapE\n",
    "    A_c,mapA = convert_to_hotvector(list(s1),a) \n",
    "    B_c,mapB = convert_to_hotvector(list(s2),b) \n",
    "       \n",
    "    vec = []\n",
    "    for l in range(len(A_c)):\n",
    "        vec.append([A_c[l],B_c[l]])\n",
    "    return vec\n",
    "\n",
    "'''\n",
    "Generate random data for evaluation\n",
    "'''\n",
    "def model_rand(num):\n",
    "    global a,c,d\n",
    "    pad = []\n",
    "    for i in range(num):\n",
    "        A = random.choices(population=a,k=20)\n",
    "        B = random.choices(population=b,k=20) #dummy\n",
    "        C = random.choices(population=c,k=50)\n",
    "        D = random.choices(population=d,k=20, weights=[0.6,0.4])\n",
    "        E = random.choices(population=e,k=5)\n",
    "        while sum(E)== 0: # at least 1 pref=1\n",
    "            E = random.choices(population=e,k=5)  \n",
    "        active_B = None\n",
    "        B_pointer = 0\n",
    "        for j in range(len(B)):\n",
    "            if active_B == None:\n",
    "                active_B = random.choice(b_cluster) \n",
    "            \n",
    "            B[j] = active_B[B_pointer]\n",
    "            if len(active_B)-1 == B_pointer:\n",
    "                active_B = None\n",
    "                B_pointer = 0\n",
    "            else:\n",
    "                B_pointer += 1\n",
    "\n",
    "        pad.append([A,B,C,D,E])\n",
    "        \n",
    "    return pad\n",
    "\n",
    "'''\n",
    "Generate 1000 Samples\n",
    "'''\n",
    "raw_text = model_rand(1000)\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab_a, n_vocab_b, n_vocab_c, n_vocab_d, n_vocab_e = len(a), len(b), len(c), len(d), len(e)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 2\n",
    "dataX = []\n",
    "dataXa = []\n",
    "dataXb = []\n",
    "dataXd = []\n",
    "dataXc = []\n",
    "dataXe = []\n",
    "dataY = []\n",
    "\n",
    "trainCategoryY = []\n",
    "trainColorY = []\n",
    "dataX_test, dataY_test = [], []\n",
    "ytrain_1 = []\n",
    "ytrain_2 = []\n",
    "for j in range(len(raw_text)):\n",
    "    for i in range(0, len(raw_text[j][0]) - seq_length, 1):\n",
    "        \n",
    "        seq_in_A = raw_text[j][0][i:i + seq_length]\n",
    "        seq_out_A = raw_text[j][0][i + seq_length]\n",
    "        \n",
    "        seq_in_B = raw_text[j][1][i:i + seq_length]\n",
    "        seq_out_B = raw_text[j][1][i + seq_length]\n",
    "        \n",
    "        seq_in_C = raw_text[j][2]\n",
    "        seq_out_C = raw_text[j][2][i + seq_length]\n",
    "        \n",
    "        seq_in_D = raw_text[j][3][i:i + seq_length]\n",
    "        seq_out_D = raw_text[j][3][i + seq_length]\n",
    "                \n",
    "        seq_in_E = raw_text[j][4][i:i + seq_length]\n",
    "       \n",
    "        v_x = getVector(seq_in_A,seq_in_B,seq_in_C,seq_in_D,seq_in_E)\n",
    "        v_y = getVectorY([seq_out_A],[seq_out_B],[seq_out_C],[seq_out_D],[])[0]\n",
    "\n",
    "        dataX_test.append([v_x[0],v_x[1],seq_in_C,v_x[3],raw_text[j][4]])\n",
    "        dataY_test.append(v_y)\n",
    "        \n",
    "        dataX.append(v_x)\n",
    "        dataXa.append(v_x[0])\n",
    "        dataXb.append(v_x[1])\n",
    "        dataXd.append(v_x[3])\n",
    "        dataXe.append(raw_text[j][4])\n",
    "        dataXc.append(seq_in_C) # known/unknown\n",
    "        \n",
    "        ytrain_1.append(v_y[0])\n",
    "        ytrain_2.append(v_y[1])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "\n",
    "ytrain_2 = np.array(ytrain_2)\n",
    "ytrain_1 = np.array(ytrain_1)\n",
    "\n",
    "Xa = np.reshape(dataXa, (n_patterns, seq_length*(len(a)), 1))\n",
    "Xb = np.reshape(dataXb, (n_patterns, seq_length*(len(b)), 1))\n",
    "Xd = np.reshape(dataXd, (n_patterns, seq_length*(len(d)), 1))\n",
    "Xc = np.reshape(dataXc, (n_patterns, (len(b)), 1)) # length b = length of known/unknown vector\n",
    "Xe = np.reshape(dataXe, (n_patterns, 5, 1))\n",
    "\n",
    "'''\n",
    "Define Model architecture\n",
    "'''\n",
    "def make_model(Xa):\n",
    "    input_a=Input(shape=(Xa.shape[1], Xa.shape[2]), name=\"Input_a\")\n",
    "    input_b=Input(shape=(Xb.shape[1], Xb.shape[2]), name=\"Input_b\")\n",
    "    input_d=Input(shape=(Xd.shape[1], Xd.shape[2]), name=\"Input_d\")\n",
    "    input_c=Input(shape=(Xc.shape[1], Xc.shape[2]), name=\"Input_c\")\n",
    "    input_e=Input(shape=(Xe.shape[1], Xe.shape[2]), name=\"Input_e\")\n",
    "    \n",
    "    embed = Embedding(input_dim=Xc.shape[1], output_dim=10, input_length=50)(input_c)\n",
    "    flat2 = Flatten()(embed)\n",
    "    \n",
    "    flat_pers = Flatten()(input_e)\n",
    "    \n",
    "    bi_lstm1 = Bidirectional(LSTM(16,return_sequences=False), name=\"LSTM_a\")(input_a)\n",
    "    bi_lstm2 = Bidirectional(LSTM(16,return_sequences=False), name=\"LSTM_b\")(input_b)\n",
    "    bi_lstm3 = Bidirectional(LSTM(16,return_sequences=False), name=\"LSTM_d\")(input_d)\n",
    "    \n",
    "    all_input = keras.layers.concatenate([bi_lstm1, bi_lstm2, bi_lstm3, flat2,flat_pers]) #bi_lstm1,\n",
    "        \n",
    "    dropout1 = Dropout(0.4, name=\"Dropout\")(all_input)\n",
    "\n",
    "    output_a = Dense(10, activation='softmax', name='output_a')(dropout1)    \n",
    "    output_b = Dense(50, activation='softmax', name='output_b')(dropout1)\n",
    "    \n",
    "    model = Model(inputs = [input_a,input_b,input_d,input_c,input_e], outputs=[output_a,output_b]) #,output_b\n",
    " \n",
    "    return model\n",
    "\n",
    "model = make_model(Xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mapping to recover categorical to classes\n",
    "'''  \n",
    "mapAx = dict((v, k) for k, v in mapA.items())\n",
    "mapBx = dict((v, k) for k, v in mapB.items())\n",
    "mapCx = dict((v, k) for k, v in mapC.items())\n",
    "mapDx = dict((v, k) for k, v in mapD.items())\n",
    "mapEx = dict((v, k) for k, v in mapE.items())\n",
    "\n",
    "'''\n",
    "Get the prediction using the model, given the input: <pattern>\n",
    "'''\n",
    "def getprediction(pattern):\n",
    "        \n",
    "    n_patterns = 1\n",
    "    Xa = np.reshape(pattern[0], (n_patterns, seq_length*(len(a)), 1))\n",
    "    Xb = np.reshape(pattern[1], (n_patterns, seq_length*(len(b)), 1))\n",
    "    Xd = np.reshape(pattern[3], (n_patterns, seq_length*(len(d)), 1))\n",
    "    Xc = np.reshape(pattern[2], (n_patterns, (len(b)), 1)) # length b = length of known/unknown vector\n",
    "    Xe = np.reshape(pattern[4], (n_patterns, 5, 1))\n",
    "    \n",
    "    prediction = model.predict([Xa,Xb,Xd,Xc,Xe], verbose=0)\n",
    "    \n",
    "    # make one-hot encoding\n",
    "    result = prediction\n",
    "    \n",
    "    r_a = list(result[0])\n",
    "    r_b = list(result[1])\n",
    "    \n",
    "    max_a = argmax(r_a)\n",
    "    max_b = argmax(r_b)\n",
    "    r_a, r_b, r_c, r_d, r_e = np.zeros((len(a),), dtype=int), np.zeros((len(b),), dtype=int), np.zeros((len(c),), dtype=int), np.zeros((len(d),), dtype=int), np.zeros((len(e),), dtype=int)\n",
    "    r_a[max_a],  r_b[max_b] = 1,1\n",
    "\n",
    "    predicted = hotvector_to_list([[r_a],[r_b]],True)\n",
    "        \n",
    "    return predicted\n",
    "\n",
    "'''\n",
    "Convert hot vector representation to list\n",
    "'''\n",
    "def hotvector_to_list(hot,ziel=False):\n",
    "    global sumabc, features\n",
    "    i = hot\n",
    "    course_A ,course_B,course_C,course_D,course_E,course_F = [],[],[],[],[],[]\n",
    "    num = 0\n",
    "    \n",
    "    #if features['A']:\n",
    "    for k in hot[0]:\n",
    "        #print('a',k[0])\n",
    "        A = mapAx[argmax(k)]\n",
    "        course_A.append(A)\n",
    "        #print(A)\n",
    "    \n",
    "    #if features['B']:\n",
    "    for k in hot[1]:\n",
    "        B = mapBx[argmax(k)]\n",
    "        course_B.append(B)\n",
    "        #print(B)\n",
    "    \n",
    "    if not ziel:\n",
    "        course_C.append(hot[2])\n",
    "        \n",
    "    if not ziel:\n",
    "        for k in hot[3]:\n",
    "            #print('d',k[3])\n",
    "            D = mapDx[argmax(k)]\n",
    "            course_D.append(D)\n",
    "            #print(D)\n",
    "  \n",
    "    num += 1\n",
    "        \n",
    "    return [course_A,course_B,course_C,course_D,course_E,course_F]\n",
    "\n",
    "'''\n",
    "Identifies whether the meshing hypothesis is fulfilled by the model\n",
    "''' \n",
    "def check_meshing(source,predict,preferences):\n",
    "    global a\n",
    "    s_eoi = source[0][0]\n",
    "    p_eoi = predict[0][0]\n",
    "    \n",
    "    hit, nhit = 0,0\n",
    "    \n",
    "    select_from = []\n",
    "    for j in range(5):\n",
    "        if preferences[j]==1: select_from.append(a[j*2:j*2+2])\n",
    "    select_from = np.concatenate(select_from)        \n",
    "    \n",
    "    # is predicted element in allowed list of methods?\n",
    "    if p_eoi in select_from:\n",
    "        hit += 1\n",
    "    else:\n",
    "        nhit += 1\n",
    "    \n",
    "    return [hit,nhit]\n",
    "\n",
    "'''\n",
    "Identifies whether the model selects only new items\n",
    "''' \n",
    "def check_on(source,predict):\n",
    "    \n",
    "    #element of interest\n",
    "    p_eoi = predict[1][0]\n",
    "    user_c = source[2][0]\n",
    "    \n",
    "    hit, nhit = 0,0\n",
    "    b_new = []\n",
    "    for i in range(len(user_c)):\n",
    "        if user_c[i] == 0:\n",
    "            b_new.append(i)\n",
    "    if p_eoi in b_new:\n",
    "        hit += 1\n",
    "    else:\n",
    "        nhit += 1\n",
    "    \n",
    "    return [hit,nhit]\n",
    "\n",
    "'''\n",
    "Identifies whether the model considers the learning progression\n",
    "''' \n",
    "def check_ros(source,predict):\n",
    "    s_eoi = source[1][1] #element of interest\n",
    "    p_eoi = predict[1][0]\n",
    "    cluster_oi = None\n",
    "    position = -1\n",
    "    hit, nhit = 0,0\n",
    "    \n",
    "    for i in b_cluster:\n",
    "        if s_eoi in i:\n",
    "            cluster_oi = i\n",
    "            position = cluster_oi.index(s_eoi)\n",
    "    if position+1 < len(cluster_oi):\n",
    "        if cluster_oi[position+1] == p_eoi:\n",
    "            hit += 1\n",
    "        else:\n",
    "            nhit += 1\n",
    "    \n",
    "    return [hit,nhit]\n",
    "\n",
    "\n",
    "def testmerge(m1,m2,m3,weight=[.33,.33,.33],show=False):    \n",
    "    model1 = load_model('model/tesla/'+m1, compile=False) \n",
    "    weights_1 = model1.get_weights()\n",
    "    \n",
    "    model2 = load_model('model/tesla/'+m2, compile=False) \n",
    "    weights_2 = model2.get_weights()\n",
    "    \n",
    "    model3 = load_model('model/tesla/'+m3, compile=False) \n",
    "    weights_3 = model3.get_weights()\n",
    "       \n",
    "\n",
    "    # overide weights of model 1 to mean of 1 and 2\n",
    "    weights_merge  = np.average( np.array([ weights_1, weights_2, weights_3 ]), axis=0 , weights=weight)\n",
    "    model.set_weights(weights_merge)\n",
    "    model.save('tesla/final/merge.hdf5')\n",
    "    \n",
    "    losses ={'output_a':keras.losses.CategoricalCrossentropy(),'output_b':keras.losses.CategoricalCrossentropy()}\n",
    "    optimizers = keras.optimizers.Adam(clipnorm=1)\n",
    "    model.compile(optimizer=optimizers, loss=losses,metrics=\"accuracy\")\n",
    "    \n",
    "    print('model compiled')\n",
    "\n",
    "    hit_all_mesh, nhit_all_mesh, anz_mesh = 0,0,0\n",
    "    hit_all_on, nhit_all_on, anz_on = 0,0,0 \n",
    "    hit_all_ros, nhit_all_ros, anz_ros = 0,0,0 \n",
    "    \n",
    "    # test with 100 samples\n",
    "    for test in range(100):\n",
    "        start = np.random.randint(0, len(dataX)-1)\n",
    "        pattern = dataX_test[start]\n",
    "        ziel = dataY_test[start]\n",
    "        source = hotvector_to_list(pattern)\n",
    "        preferences = pattern[4]\n",
    "        \n",
    "        target = hotvector_to_list([[ziel[0]],[ziel[1]]],True)\n",
    "        predicted = getprediction(pattern)\n",
    "        if show: print('Y pred: ',predicted)\n",
    "        \n",
    "        # A\n",
    "        hit_mesh,nhit_mesh = check_meshing(source,predicted,preferences)\n",
    "        if show: print('Mesh hit',hit_mesh,', nhit',nhit_mesh)\n",
    "        hit_all_mesh += hit_mesh\n",
    "        nhit_all_mesh += nhit_mesh\n",
    "        anz_mesh += 1\n",
    "        \n",
    "        # B\n",
    "        hit_on,nhit_on = check_on(source,predicted)\n",
    "        if show: print('ON hit',hit_on,', nhit',nhit_on)\n",
    "        hit_all_on += hit_on\n",
    "        nhit_all_on += nhit_on\n",
    "        if hit_on + nhit_on > 0:\n",
    "            anz_on += 1\n",
    "        \n",
    "        # B\n",
    "        hit_ros,nhit_ros = check_ros(source,predicted)\n",
    "        if show: print('ROS hit',hit_ros,', nhit',nhit_ros)\n",
    "        hit_all_ros += hit_ros\n",
    "        nhit_all_ros += nhit_ros\n",
    "        if hit_ros + nhit_ros>0:\n",
    "            anz_ros += 1\n",
    "        \n",
    "  \n",
    "        if show: print()\n",
    "        \n",
    "    print('Mesh',hit_all_mesh, nhit_all_mesh,'\\tAcc:',round(hit_all_mesh/anz_mesh,4), anz_mesh)\n",
    "    print('ON',hit_all_on, nhit_all_on,'\\tAcc:',round(hit_all_on/anz_on,4), anz_on)\n",
    "    print('ROS',hit_all_ros, nhit_all_ros,'\\tAcc:',round(hit_all_ros/anz_ros,4), anz_ros)\n",
    "    \n",
    "    print()\n",
    "    return [round(hit_all_mesh/anz_mesh,4), round(hit_all_on/anz_on,4), round(hit_all_ros/anz_ros,4)]\n",
    "\n",
    "'''\n",
    "how to use:\n",
    "[acc_e2h, acc_fl2, acc_cdfe] = testmerge('model_1.hdf5',\n",
    "                                         'model_2.hdf5',\n",
    "                                         'model_3.hdf5',\n",
    "                                         [0,1,.0], False) \n",
    "print([acc_e2h, acc_fl2, acc_cdfe])\n",
    "'''\n",
    "\n",
    "steps = .05\n",
    "res_acc = []\n",
    "all_acc_mesh = []\n",
    "all_acc_on = []\n",
    "all_acc_ros = []\n",
    "start = False\n",
    "for i in np.arange(1,-.1,-1*(steps)):\n",
    "    for j in np.arange(0,1-i+0.001,steps):\n",
    "        k = 1-i-j    \n",
    "        \n",
    "        print([round(i,2),round(j,2),abs(round(k,2))])\n",
    "        [acc_mesh, acc_on, acc_ros] = testmerge(\n",
    "            'model_1.hdf5', \n",
    "            'model_2.hdf5',  \n",
    "            'model_3.hdf5',\n",
    "            [round(i,2),round(j,2),abs(round(k))], False)\n",
    "        res_acc.append([round(i,2),round(j,2),abs(round(k,2))])\n",
    "\n",
    "        all_acc_mesh.append(acc_mesh)\n",
    "        all_acc_on.append(acc_on)\n",
    "        all_acc_ros.append(acc_ros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d86d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "res_acc_json = json.dumps([res_acc,all_acc_mesh,all_acc_on,all_acc_ros])\n",
    "with open(\"evaluation.json\", \"a\") as outfile:\n",
    "    outfile.write(res_acc_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
